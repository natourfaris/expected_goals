{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Cross-Validating a Predictive Model Using PySpark through the Lens of NHL Expected Goals\n",
    "\n",
    "### Goal and Overview\n",
    "\n",
    "This notebook aims to show how to conduct a simple machine learning project from data preparation, training and cross-validation, and choosing a model based on results from a test dataset. It is less focused on developing \"the right model\" as much as it is focused on showing *how* to choose the correct model to use. \n",
    "\n",
    "The sports analytics revolution of the past few years has given rise to new ways of evaluating players. Hockey is not an exception. Fans have been using the NHL's public data to calculate statistics that attempt to more accurately and objectively measure player performance. First came *Corsi*, that is shot attempts made by a team, whether they are a goal, saved by goaltender, miss the net, or are blocked on their way towards the net. A team that has a higher Corsi than its opponents is thought to spend more time on the attack and possessing the puck than its opponent. This indicates better quality play. Similarly, players are also tracked for Corsi statistics. In this context, Corsi refers to shot attempts, for and against a team, that occur when a player is on the ice. A player that consistently has a positive Corsi is thought to have a positive effect on play. \n",
    "\n",
    "Let's see an example of how this works: a player X is on the ice for 3 shot attempts for his team and 1 shot attempt against his team. This means that the player is positive on Corsi. This is usually expressed as a percentage, Corsi For Percentage (CF%), where we divide Corsi For by total Corsi with a player on the ice to obtain. In the case of X, CF% is 3/(3+1) = 3/4 = 75%. This player had a very good effect on his team's performance. Conversely, player Y was on the ice for 8 Corsi For and 12 Corsi Against. This translates to CF% of 8/(8+12) = 8/20 = 40%, which is not very good. In the context of the game of hockey, where players regularly change on and off the ice fluidly as the game is played without a clock stoppage, this helps us see which players had a more positive performance on their team's performance since we have many opportunities to compare team performance with and without the player being active in-game.\n",
    "\n",
    "Such a measure of course ignores many factors that differentiate different shots. It doesn't into account shot distance or the context in which the shot was made. As such, using Corsi alone to evaluate player or team performance gives us an incomplete picture.\n",
    "\n",
    "This is the role that *expected goals* play. By incorporating more information into our evaluation of team performance, we can get a more accurate picture of how a team or a player played. Expected goals are measured on a shot-by-shot basis, where the xG of a shot is the probability, given the different characteristics of the game at the time of the shot, to be a goal. The higher the xG value, the more likely a shot is to be a goal. We then use aggregate xG values to evaluate a player or a team in much the same way we evaluate them using Corsi. A player with a higher xG recorded for their team than xG against had a positive effect on play while on the ice. Since each shot's factors are mostly independent of each other, we are able to sum xG values because we are adding the the probabilities for independent events.\n",
    "\n",
    "xG works similarly as before: a player is on the ice for 0.73 xGF and 0.35 xGA. Their xGF% is thus 0.73/(0.73+0.35) = 0.73/1.08 = 67.6%. \n",
    "\n",
    "### Model Setup and How I Will Proceed\n",
    "\n",
    "How will I calculate xG? I can use a model that is calibrated on game events that allows us as much as possible to capture the game situation as a shot is taken. Based on data regarding this shot and many others like it, I can then estimate the likelihood it would have been a goal. Such a model, when trained, can be used to evaluate shots taken when a player is on the ice vs. off the ice, allowing me to estimate shot and team play quality when a player is active in-game. There are several options of models to use. One possibility is obvious from understanding what xG for any shot attempt is: it is the probability that that attempt is a goal. The easiest way to estimate the probability of an event is through logistic regression. \n",
    "\n",
    "Another possible approach is to use a tree classifier. In this instance, we can use the proportion of goals that are present at a node after running a datapoint through it as the probability of a goal. For this project, I will in fact be using a random forest classifier. The probability from a random forest is the average of each probability predicted by each tree in the forest. The advantage of a random forest model is that it allows for the possibility of nonlinear interactions between variables that might have a big impact on the probability of a goal.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data is obtained from [Moneypuck.com](moneypuck.com) who generously provide their cleaned dataset for anyone to use. Moneypuck scrapes the NHL API and uses game events to construct its dataset, which includes all unblocked shots taken by all players starting with the 2007-08 season. This data runs until the present data. At the time of writing this, the NHL has suspended its 2019-20 season to help in limiting the effects of SARS-COVID-19. We can use this season's data, along with the 2018-19 season as our testing dataset. Data from all seasons before that will be used for training and cross-validation. Once we calibrate our models on the training/CV dataset, we will choose the model that performs best on our test dataset. \n",
    "\n",
    "The variables included in the Moneypuck dataset are extensive and capture many details of what was occuring during the game at the time of the shot. They include variables such as the game score, whether either of the nets was empty, how many players were on the ice for each team (i.e. the game situation), how much time has elapsed in the game, and the shot coordinates. I will use several of these variables to estimate my model.\n",
    "\n",
    "### Libraries/Packages Used\n",
    "\n",
    "I will use PySpark for data reading and processing, as well as training, cross-validating, and testing models. I use Pandas to obtain column names.\n",
    "\n",
    "I will import the data, create several variables that we will be using in our model, and then running several models to cross-validate them. After training all models, I will evaluate them on the test dataset and choosing the one that performs best out-of-sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (IntegerType,\n",
    " StringType,DecimalType,StructType,StructField,\n",
    " ArrayType,DoubleType,FloatType)\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.feature import (StringIndexer, \n",
    "                                OneHotEncoder,\n",
    "                                OneHotEncoderEstimator, \n",
    "                                VectorAssembler)\n",
    "from pyspark.ml.classification import (LogisticRegression, \n",
    "                                       LogisticRegressionModel,\n",
    "                                      RandomForestClassifier,\n",
    "                                      RandomForestClassificationModel)\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.evaluation as evals\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "import pyspark.ml.tuning as tune\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# sc = SparkContext('local[*]')\n",
    "spark = SparkSession.builder.master('local[*]').config('spark.driver.memory','3g').appName('xG_model').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Overview of the Variables\n",
    "\n",
    "I use Pandas to read the very top of the csv file. I then use Pandas to read the column names and print them for reference. A dictionary of variables and more details is located in the data folder. I then read all columns into a Spark DataFrame as string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shotID\n",
      "1 homeTeamCode\n",
      "2 awayTeamCode\n",
      "3 season\n",
      "4 isPlayoffGame\n",
      "5 game_id\n",
      "6 homeTeamWon\n",
      "7 id\n",
      "8 time\n",
      "9 timeUntilNextEvent\n",
      "10 timeSinceLastEvent\n",
      "11 period\n",
      "12 team\n",
      "13 location\n",
      "14 event\n",
      "15 goal\n",
      "16 shotPlayContinuedOutsideZone\n",
      "17 shotPlayContinuedInZone\n",
      "18 shotGoalieFroze\n",
      "19 shotPlayStopped\n",
      "20 shotGeneratedRebound\n",
      "21 homeTeamGoals\n",
      "22 awayTeamGoals\n",
      "23 xCord\n",
      "24 yCord\n",
      "25 xCordAdjusted\n",
      "26 yCordAdjusted\n",
      "27 shotAngle\n",
      "28 shotAngleAdjusted\n",
      "29 shotAnglePlusRebound\n",
      "30 shotAngleReboundRoyalRoad\n",
      "31 shotDistance\n",
      "32 shotType\n",
      "33 shotOnEmptyNet\n",
      "34 shotRebound\n",
      "35 shotAnglePlusReboundSpeed\n",
      "36 shotRush\n",
      "37 speedFromLastEvent\n",
      "38 lastEventxCord\n",
      "39 lastEventyCord\n",
      "40 distanceFromLastEvent\n",
      "41 lastEventShotAngle\n",
      "42 lastEventShotDistance\n",
      "43 lastEventCategory\n",
      "44 lastEventTeam\n",
      "45 homeEmptyNet\n",
      "46 awayEmptyNet\n",
      "47 homeSkatersOnIce\n",
      "48 awaySkatersOnIce\n",
      "49 awayPenalty1TimeLeft\n",
      "50 awayPenalty1Length\n",
      "51 homePenalty1TimeLeft\n",
      "52 homePenalty1Length\n",
      "53 playerPositionThatDidEvent\n",
      "54 playerNumThatDidEvent\n",
      "55 playerNumThatDidLastEvent\n",
      "56 lastEventxCord_adjusted\n",
      "57 lastEventyCord_adjusted\n",
      "58 timeSinceFaceoff\n",
      "59 goalieIdForShot\n",
      "60 goalieNameForShot\n",
      "61 shooterPlayerId\n",
      "62 shooterName\n",
      "63 shooterLeftRight\n",
      "64 shooterTimeOnIce\n",
      "65 shooterTimeOnIceSinceFaceoff\n",
      "66 shootingTeamForwardsOnIce\n",
      "67 shootingTeamDefencemenOnIce\n",
      "68 shootingTeamAverageTimeOnIce\n",
      "69 shootingTeamAverageTimeOnIceOfForwards\n",
      "70 shootingTeamAverageTimeOnIceOfDefencemen\n",
      "71 shootingTeamMaxTimeOnIce\n",
      "72 shootingTeamMaxTimeOnIceOfForwards\n",
      "73 shootingTeamMaxTimeOnIceOfDefencemen\n",
      "74 shootingTeamMinTimeOnIce\n",
      "75 shootingTeamMinTimeOnIceOfForwards\n",
      "76 shootingTeamMinTimeOnIceOfDefencemen\n",
      "77 shootingTeamAverageTimeOnIceSinceFaceoff\n",
      "78 shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\n",
      "79 shootingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\n",
      "80 shootingTeamMaxTimeOnIceSinceFaceoff\n",
      "81 shootingTeamMaxTimeOnIceOfForwardsSinceFaceoff\n",
      "82 shootingTeamMaxTimeOnIceOfDefencemenSinceFaceoff\n",
      "83 shootingTeamMinTimeOnIceSinceFaceoff\n",
      "84 shootingTeamMinTimeOnIceOfForwardsSinceFaceoff\n",
      "85 shootingTeamMinTimeOnIceOfDefencemenSinceFaceoff\n",
      "86 defendingTeamForwardsOnIce\n",
      "87 defendingTeamDefencemenOnIce\n",
      "88 defendingTeamAverageTimeOnIce\n",
      "89 defendingTeamAverageTimeOnIceOfForwards\n",
      "90 defendingTeamAverageTimeOnIceOfDefencemen\n",
      "91 defendingTeamMaxTimeOnIce\n",
      "92 defendingTeamMaxTimeOnIceOfForwards\n",
      "93 defendingTeamMaxTimeOnIceOfDefencemen\n",
      "94 defendingTeamMinTimeOnIce\n",
      "95 defendingTeamMinTimeOnIceOfForwards\n",
      "96 defendingTeamMinTimeOnIceOfDefencemen\n",
      "97 defendingTeamAverageTimeOnIceSinceFaceoff\n",
      "98 defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff\n",
      "99 defendingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\n",
      "100 defendingTeamMaxTimeOnIceSinceFaceoff\n",
      "101 defendingTeamMaxTimeOnIceOfForwardsSinceFaceoff\n",
      "102 defendingTeamMaxTimeOnIceOfDefencemenSinceFaceoff\n",
      "103 defendingTeamMinTimeOnIceSinceFaceoff\n",
      "104 defendingTeamMinTimeOnIceOfForwardsSinceFaceoff\n",
      "105 defendingTeamMinTimeOnIceOfDefencemenSinceFaceoff\n",
      "106 offWing\n",
      "107 arenaAdjustedShotDistance\n",
      "108 arenaAdjustedXCord\n",
      "109 arenaAdjustedYCord\n",
      "110 arenaAdjustedYCordAbs\n",
      "111 timeDifferenceSinceChange\n",
      "112 averageRestDifference\n",
      "113 xGoal\n",
      "114 xFroze\n",
      "115 xRebound\n",
      "116 xPlayContinuedInZone\n",
      "117 xPlayContinuedOutsideZone\n",
      "118 xPlayStopped\n",
      "119 xShotWasOnGoal\n",
      "120 isHomeTeam\n",
      "121 shotWasOnGoal\n",
      "122 teamCode\n",
      "123 arenaAdjustedXCordABS\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/processed/shots_2007-2018.csv'\n",
    "data_path_2 = '../data/processed/shots_2019.csv'\n",
    "\n",
    "sample = pd.read_csv(data_path,nrows=10)\n",
    "column_names = sample.columns\n",
    "\n",
    "sample_2 = pd.read_csv(data_path_2,nrows=10)\n",
    "column_names_2 = sample_2.columns\n",
    "\n",
    "for ix,x in enumerate(column_names):\n",
    "    print(ix,x)\n",
    "\n",
    "schema = StructType([StructField(x,StringType(),True) for x in column_names])\n",
    "schema_2 = StructType([StructField(x,StringType(),True) for x in column_names_2])\n",
    "\n",
    "shots_1 = spark.read.csv(data_path,schema=schema,\n",
    "                       enforceSchema=True,header=True,ignoreLeadingWhiteSpace=True,\n",
    "                       ignoreTrailingWhiteSpace=True)\n",
    "\n",
    "shots_2 = spark.read.csv(data_path_2,schema=schema_2,\n",
    "                         enforceSchema=True,header=True,\n",
    "                        ignoreLeadingWhiteSpace=True,\n",
    "                        ignoreTrailingWhiteSpace=True)\n",
    "\n",
    "shots = shots_1.union(shots_2.select(column_names.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like a single variable to represent the skater situation for each shot. That is, does the team shooting the puck have a man advantage, or are they penalized and thus are down a skater? Intuitively, a team that has more skaters will have more control of the play, and will be more likely to score. Conversely, a team with fewer men will generally take lower quality shoty and thus be less likely to score. The *skaterDifference* variable accounts for those situations. \n",
    "\n",
    "In addition to the *skaterDifference* variable, I use several variables that capture situations where a team may be more likely to score. Below, I list the variables I will use and explain the intuition behind their inclusion.\n",
    "\n",
    "| Variable | Intuition |\n",
    "|-|-|\n",
    "| isHomeTeam                | Home team might be better able to score goals|\n",
    "| arenaAdjustedShotDistance | Closer shots are more likely to be scored|\n",
    "| adjustedShotAngle         | Shots from sharp angles are less likely to be scored|\n",
    "| shotRush                  | Shots designated as rush occur during transition and thus when defensive structure of conceding team is suboptimal|\n",
    "| shotRebound               | A shot designated as a rebound is likely to occur when a goalie is out of position or has his field of vision towards the puck obstructed|\n",
    "| shotAnglePlusReboundSpeed | A shot that occurs after a very quick change in the angle of its previous location may mean that the goaltender was forced to make a quick movement to adjust, which gives a higher chance to scoring|\n",
    "| shooterTimeOnIce          | A player that has been on ice for long may be too tired to play at their best/shoot accurately|\n",
    "| averageRestDifference     | If the shooting team is more rested than the defending team, it's more likely the latter will commit defensive lapses|\n",
    "| speedFromLastEvent        | Situations where the puck moves quickly from one location to another could mean sudden movement across the ice or into the offensive zone|\n",
    "| playerEncoder             | The position of the shooting player (C,L,R,D) |\n",
    "| shotTypeEncoder           | Different shot types may have different likelihood of being scored|\n",
    "| lastEventEncoder          | Depending on the game situation just before the event occurred, there might be events that lead to a higher probability of a goal|\n",
    "| gameSituationEncoder      | Shots made under different situations have different probabilities of being scored|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns with number of skaters for each time at time of shot attempt\n",
    "shots = shots.withColumn('shootingTeamSkaters',(shots.shootingTeamForwardsOnIce.cast(IntegerType())+\n",
    "                         shots.shootingTeamDefencemenOnIce.cast(IntegerType())))\n",
    "shots = shots.withColumn('shootingTeamSkaters',(shots.shootingTeamSkaters.cast(StringType())))\n",
    "\n",
    "shots = shots.withColumn('defendingTeamSkaters',(shots.defendingTeamForwardsOnIce.cast(IntegerType())+\n",
    "                         shots.defendingTeamDefencemenOnIce.cast(IntegerType())))\n",
    "shots = shots.withColumn('defendingTeamSkaters',(shots.defendingTeamSkaters.cast(StringType())))\n",
    "\n",
    "\n",
    "# use to create a variable for the team's strength at the time of the shot\n",
    "# shot from a 5v4 team different from one from 4v5 team\n",
    "shots = shots.withColumn('gameSituation',func.concat(shots.shootingTeamSkaters,\n",
    "                                                     func.lit('v'),\n",
    "                                                     shots.defendingTeamSkaters))\n",
    "\n",
    "full_features_list = ['isHomeTeam','arenaAdjustedShotDistance','shotAngleAdjusted','shotRush',\n",
    "                 'shotRebound','shotAnglePlusReboundSpeed','shooterTimeOnIce',\n",
    "                 'averageRestDifference','speedFromLastEvent','playerEncoder',\n",
    "                 'shotTypeEncoder','lastEventEncoder','gameSituationEncoder']\n",
    "\n",
    "# convert data type for most variables to required data type\n",
    "for col in features_list[1:-4]:\n",
    "    shots = shots.withColumn(col,shots[str(col)].cast(DecimalType()))\n",
    "\n",
    "shots = shots.withColumn('goal',shots.goal.cast(IntegerType()))\n",
    "shots = shots.withColumn('isHomeTeam',shots.isHomeTeam.cast(IntegerType()))\n",
    "shots = shots.withColumn('season',shots.season.cast(IntegerType()))\n",
    "    \n",
    "# drop rows where we don't know the position of the player \n",
    "# who made the shot; relatively very few of these so should not affect results\n",
    "shots = shots.filter('playerPositionThatDidEvent IN (\"L\",\"R\",\"C\",\"D\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to be able to be able to use categorical variables in our models,\n",
    "# we need to convert them to dummy variables\n",
    "shots_indexer = StringIndexer(inputCol='shotType',outputCol='shotTypeIndexer',\n",
    "                               handleInvalid='skip',stringOrderType='alphabetAsc')\n",
    "shots_encoder = OneHotEncoderEstimator(inputCols=['shotTypeIndexer'],outputCols=['shotTypeEncoder'])\n",
    "\n",
    "player_indexer = StringIndexer(inputCol='playerPositionThatDidEvent',outputCol='playerIndexer',\n",
    "                               handleInvalid='skip',stringOrderType='alphabetAsc')\n",
    "player_encoder = OneHotEncoderEstimator(inputCols=['playerIndexer'],outputCols=['playerEncoder'])\n",
    "\n",
    "last_event_indexer = StringIndexer(inputCol='lastEventCategory',outputCol='lastEventIndexer',\n",
    "                               handleInvalid='skip',stringOrderType='alphabetAsc')\n",
    "last_event_encoder = OneHotEncoderEstimator(inputCols=['lastEventIndexer'],outputCols=['lastEventEncoder'])\n",
    "\n",
    "game_situation_indexer = StringIndexer(inputCol='gameSituation',outputCol='gameSituationIndexer',\n",
    "                               handleInvalid='skip',stringOrderType='alphabetAsc')\n",
    "game_situation_encoder = OneHotEncoderEstimator(inputCols=['gameSituationIndexer'],outputCols=['gameSituationEncoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Predictions\n",
    "\n",
    "Usually regressions are evaluated by examining a measure like *R<sup>2</sup>* or RMSE. However, I elected to use a logarithmic scoring rule, specifically the logarithmic loss, to evaluate my predictions, as per [this](https://oddacious.github.io/proper_scoring_rules) great post. One of the features of using log loss as our evaluation metric is that it discourages overly confident predictions. Thus it incentivizes for weighing variables that have a significant effect on the probability of the result of interest: in this case, scoring a goal. \n",
    "\n",
    "Unfortunately, PySpark does not have a log loss evaluator available by default. Thus I create it below. Since we would like to *minimize* log loss, I specify that \"bigger is NOT better\" in the appropriate function. This ensures that our evaluator will use this criteria when we run on it on several iterations of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogLossEvaluator(Evaluator):\n",
    "    def __init__(self,predictionCol='probability',labelCol='label'):\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "\n",
    "    def _get_probabilities(self,row):\n",
    "        return row[1].item()\n",
    "        \n",
    "    def _evaluate(self,dataset):\n",
    "        _udf_get_probabilities = func.udf(self._get_probabilities,FloatType())\n",
    "        dataset = dataset.withColumn('prob_1',_udf_get_probabilities(dataset.select(self.predictionCol)[0]))\n",
    "        \n",
    "        self.log_loss = (-1) * dataset.select(func.sum(dataset.select(self.labelCol)[0] \n",
    "                                                           * func.log(dataset.prob_1)\n",
    "                                                       + (1 - dataset.select(self.labelCol)[0])\n",
    "                                                           * func.log(1-dataset.prob_1)\n",
    "                                                      )\n",
    "                                             ).collect()[0][0]\n",
    "        \n",
    "        return self.log_loss\n",
    "    \n",
    "    def isLargerBetter(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will specify several versions of the model and pick one that seems to perform best. I use several combinations of the variables and test them over a range of possible values for the hyperparameters of the model. For the logistic regression model in PySpark, the regularization term is:\n",
    "\n",
    "$$\\alpha(\\lambda(\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}_1)) + (1-\\alpha)(\\frac{\\lambda}{2}\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}_2^2)$$\n",
    "\n",
    "for $\\alpha\\in[0,1], \\lambda \\geq 0$. In this case, $\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}_1$ is the $L_1$ regularization term (Lasso regularization), and $\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}_2^2$ is the $L_2$ regularization term (ridge regularization). In the case where $\\lambda=0$, there is no regularization. In the case where $\\alpha=1$ and $\\lambda > 0$, we have a lasso regression, while the case of $\\alpha=0$ and $\\lambda > 0$ is a ridge regression. If $0<\\alpha<1$, we have a combination of the two regularization factors. I will test a range of values for $\\lambda$ and $\\alpha$ in my cross-validation process and determine which is the best model for each combination of features. Once the best model for each set of features has been found, I will then test the performance of each best model on the test data and select the model that performs best out-of-sample.\n",
    "\n",
    "### Training and Cross-Validation of Several Models\n",
    "\n",
    "Now that I set up my data and my evaluator, I can proceed with training and testing of the models. I train several models, testing various hyperparameter values through cross-validation. I use the default k=3 of the k-fold CrossValidator class of PySpark. What this means is that PySpark will divide the training dataset into 3 parts, or folds. It will train a model with a specific combination of hyperparameters in the parameter grid using 2 of those folds. It will then test its performance on the 3rd fold, calculating the evaluation metric (in our case the log loss) for the test fold. It will repeat this process twice more, using one of the other 2 folds as the testing fold. The log loss for this set of hyperparameters is the average log loss calculated for each fold. \n",
    "\n",
    "We do this process for each combination of $\\alpha$ and $\\lambda$ in the parameter grid. Once that's done, we find the model with the lowest log loss and designate it as the \"best model\" for a certain combination of features. This gives us the \"best foot forward\" model for every combination of features/type of model we are considering. At this point, in order to choose between these \"best foot forward\" models, we evaluate the performance of each one on the testing dataset which we have left untouched up to this point. The model that performs best on the test dataset is the model we choose as our model to use.\n",
    "\n",
    "To make things easier for us, once I find the best hyperparameter for each combination of features, I will evaluate that model on the testing data and print that value. We can compare those values at the end to determine the appropriate model. \n",
    "\n",
    "### Model Group 1: All Features\n",
    "\n",
    "For this set of models, I am testing the optimal hyperparameters for a model which contains all the features we listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45995.77407340202\n"
     ]
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=full_features_list,outputCol='features')\n",
    "pipeline = Pipeline(stages=[shots_indexer,shots_encoder,\n",
    "                            player_indexer, player_encoder,\n",
    "                            last_event_indexer,last_event_encoder,\n",
    "                            game_situation_indexer,game_situation_encoder,\n",
    "                            vector_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(shots).transform(shots)\n",
    "training = piped_data.filter('season < 2018')\n",
    "test = piped_data.filter('season >= 2018')\n",
    "# lr = LogisticRegression(labelCol='goal')\n",
    "# evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "# grid = (ParamGridBuilder()\n",
    "#            .addGrid(lr.regParam,[0,5e-4,0.1,10])\n",
    "#            .addGrid(lr.elasticNetParam,[0,0.25,0.75,1])\n",
    "#            .build())\n",
    "# cv = CrossValidator(estimator=lr,\n",
    "#                    estimatorParamMaps=grid,\n",
    "#                     evaluator=evaluator)\n",
    "\n",
    "# models = cv.fit(training)\n",
    "# full_cv_best_lr = models.bestModel\n",
    "# full_cv_best_lr.write().overwrite().save('../models/full_cv_best_lr')\n",
    "\n",
    "full_cv_best_lr = LogisticRegressionModel.load('../models/full_cv_best_lr')\n",
    "test = full_cv_best_lr.transform(test)\n",
    "evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "print(evaluator.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Group 2:\n",
    "\n",
    "Same as the above but we exclude the **playerEncoder** variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45997.73713993364\n"
     ]
    }
   ],
   "source": [
    "alt1_features_list = ['isHomeTeam','arenaAdjustedShotDistance','shotAngleAdjusted','shotRush',\n",
    "                 'shotRebound','shotAnglePlusReboundSpeed','shooterTimeOnIce',\n",
    "                 'averageRestDifference','speedFromLastEvent',\n",
    "                 'shotTypeEncoder','lastEventEncoder','gameSituationEncoder']\n",
    "\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=alt1_features_list,outputCol='features')\n",
    "pipeline = Pipeline(stages=[shots_indexer,shots_encoder,\n",
    "                            player_indexer, player_encoder,\n",
    "                            last_event_indexer,last_event_encoder,\n",
    "                            game_situation_indexer,game_situation_encoder,\n",
    "                            vector_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(shots).transform(shots)\n",
    "training = piped_data.filter('season < 2018')\n",
    "test = piped_data.filter('season >= 2018')\n",
    "\n",
    "# lr = LogisticRegression(labelCol='goal')\n",
    "# evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "# grid = (ParamGridBuilder()\n",
    "#         .addGrid(lr.regParam,[0,5e-4,0.1,10])\n",
    "#         .addGrid(lr.elasticNetParam,[0,0.25,0.75,1])\n",
    "#         .build())\n",
    "# cv = CrossValidator(estimator=lr,\n",
    "#                    estimatorParamMaps=grid,\n",
    "#                     evaluator=evaluator)\n",
    "\n",
    "# models = cv.fit(training)\n",
    "# alt1_cv_best_lr = models.bestModel\n",
    "# alt1_cv_best_lr.write().overwrite().save('../models/alt1_cv_best_lr')\n",
    "\n",
    "alt1_cv_best_lr = LogisticRegressionModel.load('../models/alt1_cv_best_lr')\n",
    "\n",
    "test = alt1_cv_best_lr.transform(test)\n",
    "evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "print(evaluator.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Group 3:\n",
    "\n",
    "This time, we're excluding both the **playerEncoder** and **shotTypeEncoder** variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46293.58923424403\n"
     ]
    }
   ],
   "source": [
    "alt2_features_list = ['isHomeTeam','arenaAdjustedShotDistance','shotAngleAdjusted','shotRush',\n",
    "                 'shotRebound','shotAnglePlusReboundSpeed','shooterTimeOnIce',\n",
    "                 'averageRestDifference','speedFromLastEvent','lastEventEncoder','gameSituationEncoder']\n",
    "\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=alt2_features_list,outputCol='features')\n",
    "pipeline = Pipeline(stages=[shots_indexer,shots_encoder,\n",
    "                            player_indexer, player_encoder,\n",
    "                            last_event_indexer,last_event_encoder,\n",
    "                            game_situation_indexer,game_situation_encoder,\n",
    "                            vector_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(shots).transform(shots)\n",
    "training = piped_data.filter('season < 2018')\n",
    "test = piped_data.filter('season >= 2018')\n",
    "\n",
    "# lr = LogisticRegression(labelCol='goal')\n",
    "# evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "# grid = (ParamGridBuilder()\n",
    "#         .addGrid(lr.regParam,[0,5e-4,0.1,10])\n",
    "#         .addGrid(lr.elasticNetParam,[0,0.25,0.75,1])\n",
    "#         .build())\n",
    "# cv = CrossValidator(estimator=lr,\n",
    "#                    estimatorParamMaps=grid,\n",
    "#                     evaluator=evaluator)\n",
    "\n",
    "# models = cv.fit(training)\n",
    "# alt2_cv_best_lr = models.bestModel\n",
    "# alt2_cv_best_lr.write().overwrite().save('../models/alt2_cv_best_lr')\n",
    "\n",
    "alt2_cv_best_lr = LogisticRegressionModel.load('../models/alt_2_cv_best_lr')\n",
    "\n",
    "test = alt2_cv_best_lr.transform(test)\n",
    "evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "print(evaluator.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Group 4:\n",
    "\n",
    "I will revert to using all features, but I will be using a random forest classifier instead of logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47764.83686744264\n"
     ]
    }
   ],
   "source": [
    "# back to using the full list of features for this model\n",
    "vector_assembler = VectorAssembler(inputCols=full_features_list,outputCol='features')\n",
    "pipeline = Pipeline(stages=[shots_indexer,shots_encoder,\n",
    "                            player_indexer, player_encoder,\n",
    "                            last_event_indexer,last_event_encoder,\n",
    "                            game_situation_indexer,game_situation_encoder,\n",
    "                            vector_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(shots).transform(shots)\n",
    "training = piped_data.filter('season < 2018')\n",
    "test = piped_data.filter('season >= 2018')\n",
    "\n",
    "# rf = RandomForestClassifier(labelCol='goal', seed=1)\n",
    "# evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "# grid = (ParamGridBuilder()\n",
    "#            .addGrid(rf.maxDepth,[3,4,5,6,7,8])\n",
    "#            .addGrid(rf.numTrees,[10,20,30])\n",
    "#            .build())\n",
    "# cv = CrossValidator(estimator=rf,\n",
    "#                    estimatorParamMaps=grid,\n",
    "#                     evaluator=evaluator)\n",
    "# models = cv.fit(training)\n",
    "# rf_cv_best_lr = models.bestModel\n",
    "# rf_cv_best_lr.write().overwrite().save('../models/rf_cv_best_lr')\n",
    "\n",
    "rf_cv_best_lr = RandomForestClassificationModel.load('../models/rf_cv_best_lr')\n",
    "\n",
    "test = rf_cv_best_lr.transform(test)\n",
    "evaluator = BinaryLogLossEvaluator(labelCol='goal')\n",
    "print(evaluator.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Sanity Check\n",
    "\n",
    "I find that the logistic regression model with all my variables performs best. I should note however that the model that excludes shooting player position (centre, winger, or defenseman) performs almost exactly as well. This implies that a player's position has almost no bearing on the probability of their scoring of a goal. What this means is once we control for different variables that affect shot quality, we can conclude that shooting talent is on average the same across these different player groups. This emphasizes the importance of shot quality and selection in scoring. The model implies that the difference in goal scoring between the average defenseman and the average forward can be attributed in large part to shot quantity and quality. \n",
    "\n",
    "Let's do a sanity check. Who were the best performing players at normal play (i.e. 5 on 5) of the 2018-2019 season?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xGoal(row):\n",
    "    return row[1].item()\n",
    "\n",
    "udf_get_xGoal = func.udf(get_xGoal,DoubleType())\n",
    "\n",
    "# recreating the data pipeline to be the one used in the first model again\n",
    "# to redo the predictions for that model\n",
    "vector_assembler = VectorAssembler(inputCols=full_features_list,outputCol='features')\n",
    "pipeline = Pipeline(stages=[shots_indexer,shots_encoder,\n",
    "                            player_indexer, player_encoder,\n",
    "                            last_event_indexer,last_event_encoder,\n",
    "                            game_situation_indexer,game_situation_encoder,\n",
    "                            vector_assembler])\n",
    "\n",
    "piped_data = pipeline.fit(shots).transform(shots)\n",
    "training = piped_data.filter('season < 2018')\n",
    "test = piped_data.filter('season >= 2018')\n",
    "\n",
    "test = full_cv_best_lr.transform(test)\n",
    "test = test.withColumn('xGoalPred',udf_get_xGoal(test.probability))\n",
    "test = test.withColumn('season',test.season.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------+--------------+\n",
      "|         shooterName|shooterPlayerId|sum(goal)|sum(xGoalPred)|\n",
      "+--------------------+---------------+---------+--------------+\n",
      "|          Timo Meier|        8478414|       26|          26.0|\n",
      "|        John Tavares|        8475166|       34|          25.1|\n",
      "|   Brendan Gallagher|        8475848|       28|          24.7|\n",
      "|        Evander Kane|        8475169|       23|          23.1|\n",
      "|        Cam Atkinson|        8474715|       27|          22.1|\n",
      "|        Tyler Seguin|        8475794|       20|          21.9|\n",
      "|     Auston Matthews|        8479318|       26|          21.3|\n",
      "|    Nathan MacKinnon|        8477492|       26|          21.2|\n",
      "|  Vladimir Tarasenko|        8475765|       24|          21.1|\n",
      "|      Brayden Schenn|        8475170|       13|          20.1|\n",
      "|        Jeff Skinner|        8475784|       27|          19.7|\n",
      "|       Ryan O'Reilly|        8475158|       25|          19.4|\n",
      "|       Sebastian Aho|        8478427|       15|          19.4|\n",
      "|   Nino Niederreiter|        8475799|       18|          19.0|\n",
      "|       Brad Marchand|        8473419|       21|          18.9|\n",
      "|   Andrei Svechnikov|        8480830|       16|          18.8|\n",
      "|     Johnny Gaudreau|        8476346|       23|          18.8|\n",
      "|     Justin Williams|        8468508|       16|          18.8|\n",
      "|       Alex Ovechkin|        8471214|       29|          18.5|\n",
      "|   Gabriel Landeskog|        8476455|       18|          18.4|\n",
      "|    Jordan Martinook|        8476921|       12|          18.3|\n",
      "|       Sidney Crosby|        8471675|       20|          18.3|\n",
      "|Jonathan Marchess...|        8476539|       17|          18.3|\n",
      "|       Jake Guentzel|        8477404|       32|          18.2|\n",
      "|       Brayden Point|        8478010|       19|          18.2|\n",
      "+--------------------+---------------+---------+--------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = test.filter('gameSituation=\"5v5\" AND season=2018').groupBy('shooterName','shooterPlayerId',).agg({'xGoalPred':'sum','goal':'sum'})\n",
    "result = result.withColumn('sum(xGoalPred)',func.round('sum(xGoalPred)',1))\n",
    "result.sort('sum(xGoalPred)',ascending=False).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how familiar you are with the NHL, this list should mostly present no surprises. There are many stars on this list, and all are players who are considered to be good offensive players. Of course, one would question whether Timo Meier would qualify as the best offensive player in the league for that season, but it is not a completely unreasonable given how close his shots tend to be to the net and how many are rebounds and tip-ins. \n",
    "\n",
    "The difference between the xGoals and actual goals is a proxy for shooting talent: a player that consistently outperforms their expected goals measure is likely to be a better shooter than average, as they can score from more difficult situations that other players find difficult to score from. Who were these players in 2018-2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+---------+--------------+----------+\n",
      "|     shooterName|shooterPlayerId|sum(goal)|sum(xGoalPred)|goalsAbove|\n",
      "+----------------+---------------+---------+--------------+----------+\n",
      "|  Leon Draisaitl|        8477934|       28|          13.9|      14.1|\n",
      "|   Jake Guentzel|        8477404|       32|          18.2|      13.8|\n",
      "|  Alex DeBrincat|        8479337|       24|          12.5|      11.5|\n",
      "|  Steven Stamkos|        8474564|       23|          11.7|      11.3|\n",
      "|   Alex Ovechkin|        8471214|       29|          18.5|      10.5|\n",
      "| Nikita Kucherov|        8476453|       24|          14.0|      10.0|\n",
      "|    Patrick Kane|        8474141|       26|          16.2|       9.8|\n",
      "|      Mark Stone|        8475913|       26|          16.5|       9.5|\n",
      "|Viktor Arvidsson|        8478042|       24|          14.6|       9.4|\n",
      "|    David Perron|        8474102|       19|           9.7|       9.3|\n",
      "|   Morgan Rielly|        8476853|       17|           7.8|       9.2|\n",
      "|      Cody Eakin|        8475236|       19|           9.9|       9.1|\n",
      "|    John Tavares|        8475166|       34|          25.1|       8.9|\n",
      "|  Brett Connolly|        8475792|       22|          13.3|       8.7|\n",
      "|    Matt Duchene|        8475168|       24|          15.4|       8.6|\n",
      "|  David Pastrnak|        8477956|       26|          17.5|       8.5|\n",
      "|  Artemi Panarin|        8478550|       20|          11.6|       8.4|\n",
      "|   Tyler Johnson|        8474870|       23|          14.9|       8.1|\n",
      "|   Logan Couture|        8474053|       23|          15.1|       7.9|\n",
      "|  Mark Scheifele|        8476460|       21|          13.4|       7.6|\n",
      "|    Ryan Dzingel|        8476288|       21|          13.4|       7.6|\n",
      "|     Jakub Vrana|        8477944|       22|          14.6|       7.4|\n",
      "|    Jeff Skinner|        8475784|       27|          19.7|       7.3|\n",
      "|    Brock Nelson|        8475754|       22|          15.0|       7.0|\n",
      "|        Max Domi|        8477503|       21|          14.1|       6.9|\n",
      "+----------------+---------------+---------+--------------+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = test.filter('gameSituation=\"5v5\" AND season=2018').groupBy('shooterName','shooterPlayerId',).agg({'xGoalPred':'sum','goal':'sum'})\n",
    "\n",
    "result = result.withColumn('goalsAbove',func.round(result.select('sum(goal)')[0]-result.select('sum(xGoalPred)')[0],1))\n",
    "result = result.withColumn('sum(xGoalPred)',func.round('sum(xGoalPred)',1))\n",
    "result.sort('goalsAbove',ascending=False).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my model, Leon Draisaitl and Jake Guentzel both exhibited excellent shooting talent in the 18-19 season. This makes sense when you consider that they spent most of their playing time in that season with Connor McDavid and Sidney Crosby respectively, two of the league's best passers and best overall players. Further on below, we see some familiar names: Alex Ovechkin, Steven Stamkos, and Patrick Kane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:retailProject]",
   "language": "python",
   "name": "conda-env-retailProject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
